## Artifact for the paper "Contract-based Resource Verification for Higher-order Functions with Memoization"

Virtual disk image containing the artifact: [popl-paper-184-artifact.tar.gz](http://lara.epfl.ch/~kandhada/popl-artifact/popl-paper-184-artifact.tar.gz)

The above vdi image is pre-installed with the following artifacts:

1. The sources and executables of the resource verification system (Leon/Orb) described in the paper.
2. Sources of all benchmarks used in the evaluation.
3. All results and data used in the evaluation and presented in Figures 9 and 10 of the paper. 

Below we provide instructions for running our resource verification system (Leon/Orb) on the benchmarks, and 
for reproducing the results of the paper. Our tool can also be tried online on some benchmarks
at [leondev.epfl.ch](http://leondev.epfl.ch) (Memresource section). 
To know more about our tool and for getting started on writing and verifying new programs with Leon/Orb
please refer to the documentation http://leondev.epfl.ch/doc/resourcebounds.html.

### Booting the Virtual Disk Image

A bootable virtual disk image with our system pre-installed is available as a part of the artifact archive.  Use the following steps to boot from the image:

1. Install [Oracle Virtual Box](https://www.virtualbox.org/wiki/Downloads). 
2. Extract the .vdi file from the archive
3. Create a new virtual machine with the downloaded .vdi file as the hard disk. We recommend at least 4GB of RAM for the virtual machine. 
4. Start the virtual machine. It will boot Ubuntu 15.04 operating system and will automatically log into the account : *paper191* (password: *oopsla*)

### Running the Grammar Tutoring System

Open a terminal and enter the following commands to run the system

1. cd /home/paper191/grammar-web
2. sbt run 
3. Open a browser and goto http://localhost:9000 which will start the online interface. The interface should resemble the one at [grammar.epfl.ch](http://grammar.epfl.ch). If any part of the interface is not rendered correctly, try switching the browser to full screen mode. We also recommend using a standard workstation monitor (such as a 20 inch display) for the application.

## Step by Step Instructions for Evaluation

### Using the Web Interface

The exercises and problems can be chosen from the pane on the right-hand-side of the interface. The interface has an editor for entering the answers to the exercises. To know the syntax of the solution expected by an exercise, select the exercise, and click on the _syntax_ button on the top left corner. Most exercises require a grammar as a solution, but the exercise _Derivation for a word_ expects a left most derivation. The system uses a very intuitive syntax for grammars. However, there are a few subtleties that are to be noted. The empty string is represented using double quotes "" and a terminal symbol that has any of the special characters: parentheses, *, +, ?, | should be enclosed within single quotes e.g. as '('. Many example grammars are also available under the exercise _Programming Language Grammars_. 
To check whether the entered solution is correct, click on _Check solution_. It will result in one of the following outcomes: (a) a counter-example or a description of why the solution is incorrect, or (b) a message saying that the solution is correct, or (c) a message that the solution "passed all test cases but was not proven to be correct", which means that the tool wasn't able to decide the correctness of the solution.
A counter-example to a solution is a string that is either accepted by the reference grammar but not by the entered solution, or generated by the solution but not accepted by the reference grammar.
In the former case the tool would output the message "The grammar does not accept the string <string> ", and in the latter case it would output the message "The grammar generates the invalid string <string>".

The interface also allows checking if the grammar entered in the editor is LL(1) or is ambiguous. The _normalize_ button eliminates epsilon productions from the grammar while preserving the language that is recognized. The _Hints_ button suggests modifications to the grammar that will prevent the counter-example, if any, found by _Check solution_. Though this feature is available for use, it is still experimental and is *not* a part of the paper.

The web interface offers an exercise on programming language grammars, which requires the users to modify a given grammar so that it does not accept the counter-examples reported. The counter-examples correspond to syntactically incorrect programs belonging to the language. The grammars shown in the exercises are obtained from sources described in the paper in Fig.13. The exercise is primarily meant as an interface for observing the problems with real-world grammars and for tweaking the grammars. 

### Reproducing Experiments on Programming Language Grammars

The following are the instructions for reproducing the results presented in Figures 14 and 15. As described in the paper, Fig. 14 reports the number of counter-examples for equivalence that were discovered by the tool in one minute on two different grammars for the same language. To generate these results, start a new shell and enter the following commands:

1. cd /home/paper191/grammar-web
2. sbt "runMain engine.Main -equivExpts ./benchmarks"

This will take about 300 seconds to complete. For every pair of grammars that are compared two files will be generated (in the same directory): a ".log" file and a ".stats" file. The log file will list all the counter-examples that were found, and the stats file is a dump of all the statistics that were collected. Two important statistics are "EquivCExs" which denotes the total number of counter-examples found, and "TimeWithoutGC" that denotes the time taken by the tool excluding the garbage collection time. (TimeWithoutGC should be ~1min for this experiment). See below for a brief explanation of all the fields of the statistics file.

### Reproducing the Results of Figure 15

As described in the paper, for this experiment we automatically generate benchmarks by injecting 3 types of errors. The benchmarks used in this experiment are available in the directory "~/grammar-web/benchmarks". (There are about 300 grammars.) Each benchmark is named as follows "grammarname-[error-type]-[num].gram", where [error-type] is a number between 1 to 3 that indicates the type of the error injected, and [num] is a number between 1 and 10. Each benchmark is compared against their original versions whose names
do not have the [error-type] and [num] fields.

To run the complete experiment execute the script: **regr.sh** from the directory "~/grammar-web". 

The script file consists of a sequence of *sbt* commands, one for each benchmark that is compared.
As before, this will produce a log and a stats file for each comparison. 
Note that this experiment will take several hours to complete. 

To test a few sample benchmarks, open the script file regr.sh and comment out the sbt commands that use the benchmarks that have to be excluded.

For comparison purposes, Fig. 15 also presents the results of the using a different tool CFGAnalyzer on the same set of benchmarks. To generate these results, run the script **cfga-regr.sh** from the directory "~/grammar-web". But, since CFGAnalyzer times out on most of the benchmarks, it may take many hours (much longer than our tool) to complete. As before, it is possible to run only a few experiments by commenting out the lines of the script that use the benchmarks that have to be excluded. The output of each comparison is dumped to a file "[benchmark-name].cfga-out.txt".

### Fields of the Statistics Files

The .stats file that are generated during comparison of grammars have the following fields each of which represents a unique metric (described below). 

* _\#EBNF-rules_ : number of productions in the input grammar. (Note that the input grammar is allowed to be in EBNF form).
* _\#rules_ and _\#nonterminals_ : number of productions and non-terminals in the grammar after conversion to BNF form.
* _WordGenCalls_ : number of words enumerated. When sampling words, this field denotes the number of samples generated.
* _EquivCExs_ : number of counter-examples that were discovered 
* _TimeWithoutGC_ : the wall clock time taken by the tool from the start to the end, excluding the time spent in garbage collection.
* _GCTime_ : time spent in garbage collection. 
* _PeakMemUsageInMB_ : Peak virtual memory usage in mega bytes.
* _AntlrParseCalls_ : number of times the Antlr v4 parser was invoked.
* _AntlrEquivCExs_ : number of counter-examples for equivalence discovered when Antlr parser was used to parse the enumerated words. 
* _CYKParseCalls_ : number of times the CYK parser was invoked. In experiments that compare programming language grammars, the CYK parser is invoked once at the end to verify the output of the Antlr parser. (Specifically, to verify whether the string rejected by the Antlr parser is not parsable.)
* _Avg.CYKParseTime_ and _Max.CYKParseTime_ : average and maximum time taken to parse a word using the CYK parser.
* _Avg.AntlrParseTime_ and _Max.AntlrParseTime_ : average and maximum time taken to parse a word using the Antlr parser.
* _Avg.time-per-call_ and _Max.time-per-call_ : average and maximum time taken to generate one word using the enumerators. When the enumerators are used in sampling mode, this field corresponds to the average and maximum time taken to generate a sample.
